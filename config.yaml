trainingInput:
  scaleTier: CUSTOM
  masterType: standard # You may set it to "large_model" if you want to increase the memory
  workerType: standard
  workerCount: 0 # You may increase the number of workers to run multiple trials in parallel
  # Documents that you should read on hypertuning configuration:
  # https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#HyperparameterSpec
  hyperparameters:
    hyperparameterMetricTag: nyc_fare
    goal: MINIMIZE # i.e., the goal is to minimize the RMSE
    # You can adjust maxTrials to balance the cost and the rounds of tuning
    maxTrials: 15
    maxParallelTrials: 6
    enableTrialEarlyStopping: TRUE
    params:
    - parameterName: max_depth
      # This hyperparamter can be used to control over-fitting,
      # because a larger depth will make the model to learn relations
      # in a more specific way for particular training samples
      #
      # This entry matches the following code in mle_trainer/train.py
      #    parser.add_argument(
      #        '--max_depth',
      #        default=6,
      #        type=int
      #    )
      type: INTEGER
      minValue: 5
      maxValue: 15
    # TODO: Add at least 3 more parameters to be tuned by HyperTune
    # You need to update both config.yaml and mle_trainer/train.py
    - parameterName: learning_rate
      type: DOUBLE
      minValue: 0.1
      maxValue: 0.5
      scaleType: UNIT_REVERSE_LOG_SCALE
    - parameterName: subsample
      type: DOUBLE
      minValue: 0.5
      maxValue: 1.0
      scaleType: UNIT_REVERSE_LOG_SCALE
    - parameterName: n_estimators
      type: INTEGER
      minValue: 100
      maxValue: 700
      scaleType: UNIT_LINEAR_SCALE
    - parameterName: min_split_loss
      type: INTEGER
      minValue: 0
      maxValue: 5
      scaleType: UNIT_LINEAR_SCALE
    - parameterName: min_child_weight
      type: INTEGER
      minValue: 0
      maxValue: 5
      scaleType: UNIT_LINEAR_SCALE
      
      
      